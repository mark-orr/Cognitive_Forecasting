Human judgments of future events, what we will call forecasts or predictions, are ubiquitous and span the mundane (e.g., will the car in front of me stop suddenly) to the extraordinary--e.g., decisions made during military operations. The scientific study of human forecasting, fittingly, draws upon myriad domains and literature. The focus of the current report is what is called prediction of event duration, a specific domain that is defined by forecasting how much longer an event will continue in reference to how long it has lasted thus far.  Examples are:   "We've been at war for 2 months; how much longer will the war last?"  "A train has been traveling for 20 minutes; what will be the total time of the trip?"
 
In this article, we explore event duration prediction in the case of real-world epidemics.   We observed two rounds of an online forecasting tournament focused on characteristics of the COVID-19 epidemic in Virginia from late 2021 to early 2022 (coinciding with the Omicron wave) during which participants were asked to judge the timing of the peak of the epidemic.  

From a psychological perspective, the data were impoverished offering little beyond the actual predictions.  Thus, our methodological approach stemmed from what is called rational analysis \citep{anderson2013adaptive, Lieder2020, marr2010vision}, a useful first step in understanding the psychological processes and representations that drive a behavior.  Simply put, rational analysis puts forth an optimal computation (a rational model) in relation to a task and addresses the degree to which human performance matches the optimal computation (see \cite{Tauber2017} for detailed exposition on this notion and related alternatives).  

Rational models have shown promise in explaining event duration prediction:  (i) humans can estimate the an event's duration in an optimal way given little context and (ii) prior knowledge can be combined with new, observed data in an optimal way \citep{GriffithsTenenbaum2006,GriffithsTenenbaum2011,Tauber2017}.  (These results come with some degree of theoretical contention, e.g., \cite{MozerPashlerHomaei2008}).  The basis of these models, the one we adapted in our methods, is a Bayesian decision model derived from: 
\begin{equation}\label{eq:posteriorfori}
    P(t_{total} \vert t_{past}) = \frac{P(t_{past} \vert t_{total}) P(t_{total})} {P(t_{past})}
\end{equation}
where $t_{total}$ is a particular value of an event's \textit{total} duration, coming from a distribution of durations; $t_{past}$ is the duration at which a decision is made, or if post-hoc, the point for which the decision is made; $P(X)$ stands for probability of $X$.  We can define the decision function as the expected value of the probability distribution generated from Equation \ref{eq:posteriorfori} over the distribution of all $t_{total}$. We call the output of this function, $t_{predicted}$, to capture the notion that the rational model predicts a value of $t$ to be the duration of the event in question.  

If we define the Bayesian prediction function (borrowing from \cite{GriffithsTenenbaum2006}) as $t_{predicted}$ over all values of $t_{past}$,  we see that it accords with human predictions under experimental conditions \citep{GriffithsTenenbaum2006} in which human predictions are provided in reference to a single event (e.g., how much longer will someone live if they are now 55 years old).  Further, experimental evidence supports the hypothesis that human event duration prediction is affected by both prior, durable knowledge and new observations; in terms of the the Bayesian decision model the new observations are integrated into the likelihood function $P(t_{past} \vert t_{total})$ \citep{GriffithsTenenbaum2011}.

We apply the Bayesian decision model to an interesting real-time, real world context.  In the forecasting tournament that generated our data, participants were allowed to make repeated predictions over time, during a single event (in this case the event was the Omicron wave of COVID-19).  This observational context--uncontrolled repeated observations in real-time--was useful because it afforded a unique, applied context for understanding some key implications of the rational decision model.  Specifically, it surfaced two important issues. 

First, the observational context had the potentiality of eliciting $t_{predicted}$ as $t_{past}$ approaches $t_{predicted}$ from participants.  From the perspective of the Bayesian decision model, this context brings to the fore a seemingly paradoxical conclusion of the rational model, one that is inherent in its construction.  When an agent is invariant in its prediction over time ($t_{predicted}$), the expected value of the prior distribution must decrease as $t_{past}$ approaches $t_{predicted}$\footnote{We will provide details of this result in the Methods section.}.   This reduction in prior, what we call the \textit{prior crash}, is paradoxical not because priors must be fixed but because the change in prior would be self-organized, without information external to the model.  Rapid changes in the prior are typically assumed to be driven by changes in the environment (see \cite{Sohn2021,Prat2021} for examples).

Its useful to contrast this with another limiting case.  When the prior is fixed (instead of the prediction), the prediction $t_{predicted}$ increases as $t_{past}$ approaches it.  Taken together, the limiting cases, the invariant prediction and the invariant prior, are just flip-sides of the same model. The first case forces the prior downward to maintain a constant prediction; the second case forces the prediction upward to maintained an invariant prior. 

The paradoxical feature of the rational decision model is an issue only to the degree that is observed, something that is not guaranteed. As $t_{past}$ approaches $t_{predicted}$, it is possible that, in real-time contexts, humans adapt their predictions, either in an effort to minimize the degree of change in the prior or some other motive (we leave the motive for the discussion). So, the first objective of our work was to \textit{observe how participants adapt their predictions in real-time and to understand the relation of the un-observable theoretic component, the prior, to the human predictions}.  

The second issue raised by our observational context focuses on the role of information external to the rational decision model.  The participants in the forecasting tournament were not limited in terms of the kinds of information used to make predictions.  An obvious candidate was the COVID-19 daily-case count reported in the State of Virginia (the forecasting tournament was focused on Virginia). In fact, this data source was provided to participants as a potential source of information in the context of the forecasting question (see \cite{VDHOnline} for the current daily-case count data in Virginia). Thus, our second objective was \textit{to explore possible relations between external data and the human predictions}. If the human predictions are correlated in some manner with the epidemiological case-counts, then it suggests a possible re-consideration of the rational model described above, depending on the nature of the relation.  A priori, it is not obvious how to integrate such information into the rational model as it is specified above.

